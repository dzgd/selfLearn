{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####核主成分分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"C:\\Users\\Administrator\\Desktop\\table1_end_data.csv\")\n",
    "data = data.drop(columns = \"id\")\n",
    "x = data.iloc[:,1:]\n",
    "y = data.iloc[:,0]\n",
    "kernels=['linear','poly']\n",
    "mse_linear = []\n",
    "mse_poly = []\n",
    "for i in kernels:\n",
    "    for j in range(2,30):\n",
    "        kpca=decomposition.KernelPCA(n_components=j,kernel=i)\n",
    "        kpca = kpca.fit_transform(x)\n",
    "        once = cross_val_score(RFR(n_estimators=20,random_state= 0 ),kpca,y,scoring = \"neg_mean_squared_error\",cv=10).mean()\n",
    "        if i == \"linear\" :\n",
    "            mse_linear.append(once * -1)\n",
    "        if i == \"poly\" :\n",
    "            mse_poly.append(once * -1)\n",
    "      \n",
    "\n",
    "color = [\"red\" , \"blue\"]       \n",
    "fig = plt.figure(figsize=[20,5])\n",
    "mse = [mse_linear , mse_poly ]\n",
    "mse_char = [\"mse_linear\" , \"mse_poly\" ]\n",
    "for i ,col in enumerate(mse) :\n",
    "    ax = fig.add_subplot(1,2,i+1)## 两行两列，每个单元显示核函数为 rbf 的 KernelPCA 一组参数的效果图\n",
    "    ax.plot(range(2,30), col ,color = color[i] ,label = mse_char[i])\n",
    "    ax.set_xlabel(\"number of components after dimension reduction\")\n",
    "    ax.set_ylabel(mse_char[i])\n",
    "    ax.legend(loc=\"best\")\n",
    "    n = np.argmin(col)+2\n",
    "    print(\"{}的mse最小时所需要的降维为{},均方误差最小为{}\".format(mse_char[i],n,col[n-2]))\n",
    "plt.suptitle(\"linear or poly of KPCA\")\n",
    "plt.show()          \n",
    "            \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "fig = plt.figure(figsize=[20,5])\n",
    "colors = [\"red\" ,\"yellow\" ,\"green\" , \"blue\"]\n",
    "Gammas=[0.1,0.5,1,4]\n",
    "Gammas_char=[\"gamma = 0.1\",\"gamma = 0.5\",\"gamma = 1\",\"gamma = 4\"]# rbf 核的参数组成的列表。每个参数就是 gamma值\n",
    "for i,gamma in enumerate(Gammas):\n",
    "    mse_rbf = []\n",
    "    for j in range(1,10):\n",
    "        kpca=decomposition.KernelPCA(n_components=j,kernel='rbf',gamma=gamma)\n",
    "        kpca = kpca.fit_transform(x)\n",
    "        once = cross_val_score(RFR(n_estimators=20,random_state= 0 ),kpca,y,scoring = \"neg_mean_squared_error\",cv=10).mean()\n",
    "        mse_rbf.append(once * -1)\n",
    "    ax = fig.add_subplot(2,2,i+1)## 两行两列，每个单元显示核函数为 rbf 的 KernelPCA 一组参数的效果图\n",
    "    ax.plot(range(1,10), mse_rbf ,color = colors[i] ,label = Gammas_char[i])\n",
    "    ax.set_xlabel(\"number of components after dimension reduction \")\n",
    "    ax.set_ylabel(\"mse_rbf\")\n",
    "    ax.legend(loc=\"best\")\n",
    "    n = np.argmin(mse_rbf)+1\n",
    "    print(\"{}的mse最小时所需要的降维为{},均方误差最小为{}\".format(Gammas_char[i],n,mse_rbf[n-1]))\n",
    "plt.suptitle(\"KPCA-rbf\")\n",
    "plt.show()\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "#####已经确定选用核函数为poly时用26个变量进行降维处理并导出#############    \n",
    "    \n",
    "kpca=decomposition.KernelPCA(n_components=26,kernel=\"poly\")\n",
    "kpca = kpca.fit_transform(x)\n",
    "var_poly = [\"var1\",\"var2\",\"var3\",\"var4\",\"var5\",\"var6\",\"var7\",\"var8\",\"var9\"\n",
    "       ,\"var10\",\"var11\",\"var12\",\"var13\",\"var14\",\"var15\",\"var16\",\"var7\"\n",
    "       ,\"var18\",\"var19\",\"var20\",\"var21\",\"var22\",\"var23\",\"var24\",\"var25\"\n",
    "       ,\"var26\"]\n",
    "x_fe_poly = pd.DataFrame(kpca,columns = var_poly )\n",
    "x_fe_poly.to_csv(r\"C:\\Users\\Administrator\\Desktop\\x_fe_poly.csv\")\n",
    "fit_26 = RFR(n_estimators=20,random_state= 0).fit(x_fe_poly,y)\n",
    "features = x_fe_poly.columns\n",
    "importances = fit_26.feature_importances_\n",
    "indices = np.argsort(importances[0:10])  # top 10 features\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), importances[indices], color='g', align='center')\n",
    "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dropout, Dense, SimpleRNN\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import math\n",
    "data = pd.read_csv(r\"C:\\Users\\Administrator\\Desktop\\x_fe_poly.csv\")\n",
    "data = data.sort_values('id', ascending = False)\n",
    "\n",
    "\n",
    "training_set = data.iloc[0:325 - 50, : ].values  # 前(325-50=275)天的数据作为训练集,表格从0开始计数\n",
    "test_set = data.iloc[325 - 50: , : ].values  # 后50天的数据作为测试集\n",
    "\n",
    "# 归一化\n",
    "sc_x = MinMaxScaler(feature_range=(0, 1))  # 定义归一化：归一化到(0，1)之间\n",
    "sc_y = MinMaxScaler(feature_range=(0, 1))  # 定义归一化：归一化到(0，1)之间\n",
    "training_set_scaled_x = sc_x.fit_transform(training_set[:,2:])  # 求得训练集的最大值，最小值这些训练集固有的属性，并在训练集上进行归一化\n",
    "training_set_scaled_y = sc_y.fit_transform(training_set[:,1:2])  # 求得训练集的最大值，最小值这些训练集固有的属性，并在训练集上进行归一化\n",
    "test_set_x = sc_x.transform(test_set[:,2:])  # 利用训练集的属性对测试集进行归一化\n",
    "test_set_y = sc_y.transform(test_set[:,1:2])  # 利用训练集的属性对测试集进行归一化\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "# 测试集：csv表格中前325-50=275个数据\n",
    "# 利用for循环，遍历整个训练集，提取训练集中连续5天的26个降维新变量作为输入特征x_train，第6天的RON_LOSS作为标签，for循环共构建325-50-5=270组数据。\n",
    "\n",
    "for i in range(5, len(training_set_scaled_x)):\n",
    "    x_train.append(training_set_scaled_x[i - 5:i, :])\n",
    "    y_train.append(training_set_scaled_y[i, :])\n",
    "# 对训练集进行打乱\n",
    "np.random.seed(7)\n",
    "np.random.shuffle(x_train)\n",
    "np.random.seed(7)\n",
    "np.random.shuffle(y_train)\n",
    "tf.random.set_seed(7)\n",
    "# 将训练集由list格式变为array格式\n",
    "x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "\n",
    "# 使x_train符合RNN输入要求：[送入样本数， 循环核时间展开步数， 每个时间步输入特征个数]。\n",
    "# 此处整个数据集送入，送入样本数为x_train.shape[0]即270组数据；输入连续5个的26个降维新变量，预测出第6天的RON_LOSS，循环核时间展开步数为5; 每个时间步送入的特征是连续5天的26个降维新变量，有26个数据，故每个时间步输入特征个数为26\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], 5, 26))\n",
    "# 测试集：csv表格中后50天数据\n",
    "# 利用for循环，遍历整个测试集，提取测试集中连续5天的26个降维新变量作为输入特征x_train，第6天的数据作为RON_LOSS，for循环共构建50-5=45组数据。\n",
    "for i in range(5, len(test_set_x)):\n",
    "    x_test.append(test_set_x[i - 5 : i, :])\n",
    "    y_test.append(test_set_y[i, :])\n",
    "# 测试集变array并reshape为符合RNN输入要求：[送入样本数， 循环核时间展开步数， 每个时间步输入特征个数]\n",
    "x_test, y_test = np.array(x_test), np.array(y_test)\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], 5, 26))\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    SimpleRNN(80, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    SimpleRNN(100),\n",
    "    Dropout(0.2),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "              loss='mean_squared_error')  # 损失函数用均方误差\n",
    "# 该应用只观测loss数值，不观测准确率，所以删去metrics选项，一会在每个epoch迭代显示时只显示loss值\n",
    "#C:\\Users\\Administrator\\Desktop\\\n",
    "checkpoint_save_path = r\"C:\\Users\\Administrator\\Desktop\\RNNcheckpoint\\RNN_RON_LOSS.ckpt\"\n",
    "\n",
    "if os.path.exists(checkpoint_save_path + '.index'):\n",
    "    print('-------------load the model-----------------')\n",
    "    model.load_weights(checkpoint_save_path)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_save_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 save_best_only=True,\n",
    "                                                 monitor='val_loss')\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=64, epochs=50, validation_data=(x_test, y_test), validation_freq=1,\n",
    "                    callbacks=[cp_callback])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "file = open('C:/Users/Administrator/Desktop/RNN_RON_LOSS_weights.txt', 'w' )  # 参数提取\n",
    "for v in model.trainable_variables:\n",
    "    file.write(str(v.name) + '\\n')\n",
    "    file.write(str(v.shape) + '\\n')\n",
    "    file.write(str(v.numpy()) + '\\n')\n",
    "file.close()\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "################## predict ######################\n",
    "# 测试集输入模型进行预测\n",
    "predicted_RON_LOSS = model.predict(x_test)\n",
    "# 对预测数据还原---从（0，1）反归一化到原始范围\n",
    "predicted_RON_LOSS = sc_y.inverse_transform(predicted_RON_LOSS)\n",
    "# 对真实数据还原---从（0，1）反归一化到原始范围\n",
    "real_RON_LOSS = sc_y.inverse_transform(y_test)\n",
    "# 画出真实数据和预测数据的对比曲线\n",
    "plt.plot(real_RON_LOSS, color='red', label=' realed RON LOSS')\n",
    "plt.plot(predicted_RON_LOSS, color='blue', label='Predicted RON LOSS')\n",
    "plt.title('RON LOSS Prediction')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('RON LOSS')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "##########evaluate##############\n",
    "# calculate MSE 均方误差 ---> E[(预测值-真实值)^2] (预测值减真实值求平方后求均值)\n",
    "mse = mean_squared_error(predicted_RON_LOSS, real_RON_LOSS)\n",
    "# calculate RMSE 均方根误差--->sqrt[MSE]    (对均方误差开方)\n",
    "rmse = math.sqrt(mean_squared_error(predicted_RON_LOSS, real_RON_LOSS))\n",
    "# calculate MAE 平均绝对误差----->E[|预测值-真实值|](预测值减真实值求绝对值后求均值）\n",
    "mae = mean_absolute_error(predicted_RON_LOSS, real_RON_LOSS)\n",
    "print('均方误差: %.6f' % mse)\n",
    "print('均方根误差: %.6f' % rmse)\n",
    "print('平均绝对误差: %.6f' % mae)\n",
    "\n",
    "data = pd.read_csv(r\"C:\\Users\\Administrator\\Desktop\\x_fe_poly.csv\")\n",
    "data = data.sort_values('id', ascending = False)\n",
    "all_samples = data.iloc[: , : ].values  # 所有样本的特征作为测试集\n",
    "\n",
    "# 归一化\n",
    "sc_x = MinMaxScaler(feature_range=(0, 1))  # 定义归一化：归一化到(0，1)之间\n",
    "sc_y = MinMaxScaler(feature_range=(0, 1))  # 定义归一化：归一化到(0，1)之间\n",
    "test_set_scaled_x = sc_x.fit_transform(all_samples[:,2:])  # 求得训练集的最大值，最小值这些训练集固有的属性，并在训练集上进行归一化\n",
    "test_set_scaled_y = sc_y.fit_transform(all_samples[:,1:2])  # 求得训练集的最大值，最小值这些训练集固有的属性，并在训练集上进行归一化\n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "for i in range(5, len(test_set_scaled_x)):\n",
    "    x_test.append(test_set_scaled_x[i - 5:i, :])\n",
    "    y_test.append(test_set_scaled_x[i, :])\n",
    "\n",
    "# 将训练集由list格式变为array格式\n",
    "x_test, y_test = np.array(x_test), np.array(y_test)\n",
    "\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], 5, 26))\n",
    "\n",
    "################## predict ######################\n",
    "# 测试集输入模型进行预测\n",
    "predicted_RON_LOSS = model.predict(x_test)\n",
    "# 对预测数据还原---从（0，1）反归一化到原始范围\n",
    "predicted_RON_LOSS = sc_y.inverse_transform(predicted_RON_LOSS)\n",
    "# 对真实数据还原---从（0，1）反归一化到原始范围\n",
    "real_RON_LOSS = all_samples[5:,1:2]\n",
    "\n",
    "# 画出真实数据和预测数据的对比曲线\n",
    "plt.plot(range(320,0,-1),real_RON_LOSS.ravel(), color='red', label=' realed RON LOSS')\n",
    "plt.plot(range(320,0,-1),predicted_RON_LOSS.ravel(), color='blue', label='Predicted RON LOSS')\n",
    "plt.title(' RNN of RON LOSS Prediction')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('RON LOSS')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dropout, Dense, LSTM\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "data = pd.read_csv(r\"C:\\Users\\Administrator\\Desktop\\x_fe_poly.csv\")\n",
    "data = data.sort_values('id', ascending = False)\n",
    "\n",
    "\n",
    "training_set = data.iloc[0:325 - 50, : ].values  # 前(2426-300=2126)天的开盘价作为训练集,表格从0开始计数，2:3 是提取[2:3)列，前闭后开,故提取出C列开盘价\n",
    "test_set = data.iloc[325 - 50: , : ].values  # 后50天的开盘价作为测试集\n",
    "\n",
    "# 归一化\n",
    "sc_x = MinMaxScaler(feature_range=(0, 1))  # 定义归一化：归一化到(0，1)之间\n",
    "sc_y = MinMaxScaler(feature_range=(0, 1))  # 定义归一化：归一化到(0，1)之间\n",
    "training_set_scaled_x = sc_x.fit_transform(training_set[:,2:])  # 求得训练集的最大值，最小值这些训练集固有的属性，并在训练集上进行归一化\n",
    "training_set_scaled_y = sc_y.fit_transform(training_set[:,1:2])  # 求得训练集的最大值，最小值这些训练集固有的属性，并在训练集上进行归一化\n",
    "test_set_x = sc_x.transform(test_set[:,2:])  # 利用训练集的属性对测试集进行归一化\n",
    "test_set_y = sc_y.transform(test_set[:,1:2])  # 利用训练集的属性对测试集进行归一化\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "\n",
    "# 测试集：csv表格中前325-50=275个数据\n",
    "# 利用for循环，遍历整个训练集，提取训练集中连续5天的26个降维新变量作为输入特征x_train，第6天的RON_LOSS作为标签，for循环共构建325-50-5=270组数据。\n",
    "\n",
    "for i in range(5, len(training_set_scaled_x)):\n",
    "    x_train.append(training_set_scaled_x[i - 5:i, :])\n",
    "    y_train.append(training_set_scaled_y[i, :])\n",
    "# 对训练集进行打乱\n",
    "np.random.seed(7)\n",
    "np.random.shuffle(x_train)\n",
    "np.random.seed(7)\n",
    "np.random.shuffle(y_train)\n",
    "tf.random.set_seed(7)\n",
    "# 将训练集由list格式变为array格式\n",
    "x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "\n",
    "# 使x_train符合RNN输入要求：[送入样本数， 循环核时间展开步数， 每个时间步输入特征个数]。\n",
    "# 此处整个数据集送入，送入样本数为x_train.shape[0]即270组数据；输入连续5个的26个降维新变量，预测出第6天的RON_LOSS，循环核时间展开步数为5; 每个时间步送入的特征是连续5天的26个降维新变量，有26个数据，故每个时间步输入特征个数为26\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], 5, 26))\n",
    "# 测试集：csv表格中后50天数据\n",
    "# 利用for循环，遍历整个测试集，提取测试集中连续5天的26个降维新变量作为输入特征x_train，第6天的数据作为RON_LOSS，for循环共构建50-5=45组数据。\n",
    "for i in range(5, len(test_set_x)):\n",
    "    x_test.append(test_set_x[i - 5 : i, :])\n",
    "    y_test.append(test_set_y[i, :])\n",
    "# 测试集变array并reshape为符合RNN输入要求：[送入样本数， 循环核时间展开步数， 每个时间步输入特征个数]\n",
    "x_test, y_test = np.array(x_test), np.array(y_test)\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], 5, 26))\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    LSTM(80, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(100),\n",
    "    Dropout(0.2),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "              loss='mean_squared_error')  # 损失函数用均方误差\n",
    "# 该应用只观测loss数值，不观测准确率，所以删去metrics选项，一会在每个epoch迭代显示时只显示loss值\n",
    "#C:\\Users\\Administrator\\Desktop\\\n",
    "checkpoint_save_path = r\"C:\\Users\\Administrator\\Desktop\\LSTMcheckpoint\\LSTM_RON_LOSS.ckpt\"\n",
    "\n",
    "if os.path.exists(checkpoint_save_path + '.index'):\n",
    "    print('-------------load the model-----------------')\n",
    "    model.load_weights(checkpoint_save_path)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_save_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 save_best_only=True,\n",
    "                                                 monitor='val_loss')\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=64, epochs=50, validation_data=(x_test, y_test), validation_freq=1,\n",
    "                    callbacks=[cp_callback])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "file = open('C:/Users/Administrator/Desktop/LSTM_RON_LOSS_weights.txt', 'w' )  # 参数提取\n",
    "for v in model.trainable_variables:\n",
    "    file.write(str(v.name) + '\\n')\n",
    "    file.write(str(v.shape) + '\\n')\n",
    "    file.write(str(v.numpy()) + '\\n')\n",
    "file.close()\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "################## predict ######################\n",
    "# 测试集输入模型进行预测\n",
    "predicted_RON_LOSS = model.predict(x_test)\n",
    "# 对预测数据还原---从（0，1）反归一化到原始范围\n",
    "predicted_RON_LOSS = sc_y.inverse_transform(predicted_RON_LOSS)\n",
    "# 对真实数据还原---从（0，1）反归一化到原始范围\n",
    "real_RON_LOSS = sc_y.inverse_transform(y_test)\n",
    "# 画出真实数据和预测数据的对比曲线\n",
    "plt.plot(real_RON_LOSS, color='red', label=' realed RON LOSS')\n",
    "plt.plot(predicted_RON_LOSS, color='blue', label='Predicted RON LOSS')\n",
    "plt.title('RON LOSS Prediction')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('RON LOSS')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "##########evaluate##############\n",
    "# calculate MSE 均方误差 ---> E[(预测值-真实值)^2] (预测值减真实值求平方后求均值)\n",
    "mse = mean_squared_error(predicted_RON_LOSS, real_RON_LOSS)\n",
    "# calculate RMSE 均方根误差--->sqrt[MSE]    (对均方误差开方)\n",
    "rmse = math.sqrt(mean_squared_error(predicted_RON_LOSS, real_RON_LOSS))\n",
    "# calculate MAE 平均绝对误差----->E[|预测值-真实值|](预测值减真实值求绝对值后求均值）\n",
    "mae = mean_absolute_error(predicted_RON_LOSS, real_RON_LOSS)\n",
    "print('均方误差: %.6f' % mse)\n",
    "print('均方根误差: %.6f' % rmse)\n",
    "print('平均绝对误差: %.6f' % mae)\n",
    "\n",
    "\n",
    "data = pd.read_csv(r\"C:\\Users\\Administrator\\Desktop\\x_fe_poly.csv\")\n",
    "data = data.sort_values('id', ascending = False)\n",
    "all_samples = data.iloc[: , : ].values  # 所有样本的特征作为测试集\n",
    "\n",
    "# 归一化\n",
    "sc_x = MinMaxScaler(feature_range=(0, 1))  # 定义归一化：归一化到(0，1)之间\n",
    "sc_y = MinMaxScaler(feature_range=(0, 1))  # 定义归一化：归一化到(0，1)之间\n",
    "test_set_scaled_x = sc_x.fit_transform(all_samples[:,2:])  # 求得训练集的最大值，最小值这些训练集固有的属性，并在训练集上进行归一化\n",
    "test_set_scaled_y = sc_y.fit_transform(all_samples[:,1:2])  # 求得训练集的最大值，最小值这些训练集固有的属性，并在训练集上进行归一化\n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "for i in range(5, len(test_set_scaled_x)):\n",
    "    x_test.append(test_set_scaled_x[i - 5:i, :])\n",
    "    y_test.append(test_set_scaled_x[i, :])\n",
    "\n",
    "# 将训练集由list格式变为array格式\n",
    "x_test, y_test = np.array(x_test), np.array(y_test)\n",
    "\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], 5, 26))\n",
    "\n",
    "################## predict ######################\n",
    "# 测试集输入模型进行预测\n",
    "predicted_RON_LOSS = model.predict(x_test)\n",
    "# 对预测数据还原---从（0，1）反归一化到原始范围\n",
    "predicted_RON_LOSS = sc_y.inverse_transform(predicted_RON_LOSS)\n",
    "# 对真实数据还原---从（0，1）反归一化到原始范围\n",
    "real_RON_LOSS = all_samples[5:,1:2]\n",
    "\n",
    "# 画出真实数据和预测数据的对比曲线\n",
    "plt.plot(range(320,0,-1),real_RON_LOSS.ravel(), color='red', label=' realed RON LOSS')\n",
    "plt.plot(range(320,0,-1),predicted_RON_LOSS.ravel(), color='blue', label='Predicted RON LOSS')\n",
    "plt.title(' LSTM of RON LOSS Prediction')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('RON LOSS')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dropout, Dense, GRU\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import math\n",
    "\n",
    "\n",
    "data = pd.read_csv(r\"C:\\Users\\Administrator\\Desktop\\x_fe_poly.csv\")\n",
    "data = data.sort_values('id', ascending = False)\n",
    "\n",
    "\n",
    "training_set = data.iloc[0:325 - 50, : ].values  # 前(2426-300=2126)天的开盘价作为训练集,表格从0开始计数，2:3 是提取[2:3)列，前闭后开,故提取出C列开盘价\n",
    "test_set = data.iloc[325 - 50: , : ].values  # 后50天的开盘价作为测试集\n",
    "\n",
    "# 归一化\n",
    "sc_x = MinMaxScaler(feature_range=(0, 1))  # 定义归一化：归一化到(0，1)之间\n",
    "sc_y = MinMaxScaler(feature_range=(0, 1))  # 定义归一化：归一化到(0，1)之间\n",
    "training_set_scaled_x = sc_x.fit_transform(training_set[:,2:])  # 求得训练集的最大值，最小值这些训练集固有的属性，并在训练集上进行归一化\n",
    "training_set_scaled_y = sc_y.fit_transform(training_set[:,1:2])  # 求得训练集的最大值，最小值这些训练集固有的属性，并在训练集上进行归一化\n",
    "test_set_x = sc_x.transform(test_set[:,2:])  # 利用训练集的属性对测试集进行归一化\n",
    "test_set_y = sc_y.transform(test_set[:,1:2])  # 利用训练集的属性对测试集进行归一化\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "# 测试集：csv表格中前325-50=275个数据\n",
    "# 利用for循环，遍历整个训练集，提取训练集中连续5天的26个降维新变量作为输入特征x_train，第6天的RON_LOSS作为标签，for循环共构建325-50-5=270组数据。\n",
    "\n",
    "for i in range(5, len(training_set_scaled_x)):\n",
    "    x_train.append(training_set_scaled_x[i - 5:i, :])\n",
    "    y_train.append(training_set_scaled_y[i, :])\n",
    "# 对训练集进行打乱\n",
    "np.random.seed(7)\n",
    "np.random.shuffle(x_train)\n",
    "np.random.seed(7)\n",
    "np.random.shuffle(y_train)\n",
    "tf.random.set_seed(7)\n",
    "# 将训练集由list格式变为array格式\n",
    "x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "\n",
    "# 使x_train符合RNN输入要求：[送入样本数， 循环核时间展开步数， 每个时间步输入特征个数]。\n",
    "# 此处整个数据集送入，送入样本数为x_train.shape[0]即270组数据；输入连续5个的26个降维新变量，预测出第6天的RON_LOSS，循环核时间展开步数为5; 每个时间步送入的特征是连续5天的26个降维新变量，有26个数据，故每个时间步输入特征个数为26\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], 5, 26))\n",
    "# 测试集：csv表格中后50天数据\n",
    "# 利用for循环，遍历整个测试集，提取测试集中连续5天的26个降维新变量作为输入特征x_train，第6天的RON_LOSS作为标签，for循环共构建50-5=45组数据。\n",
    "for i in range(5, len(test_set_x)):\n",
    "    x_test.append(test_set_x[i - 5 : i, :])\n",
    "    y_test.append(test_set_y[i, :])\n",
    "# 测试集变array并reshape为符合RNN输入要求：[送入样本数， 循环核时间展开步数， 每个时间步输入特征个数]\n",
    "x_test, y_test = np.array(x_test), np.array(y_test)\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], 5, 26))\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    GRU(80, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    GRU(100),\n",
    "    Dropout(0.2),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "              loss='mean_squared_error')  # 损失函数用均方误差\n",
    "# 该应用只观测loss数值，不观测准确率，所以删去metrics选项，一会在每个epoch迭代显示时只显示loss值\n",
    "#C:\\Users\\Administrator\\Desktop\\\n",
    "checkpoint_save_path = r\"C:\\Users\\Administrator\\Desktop\\GRUcheckpoint\\GRU_RON_LOSS.ckpt\"\n",
    "\n",
    "if os.path.exists(checkpoint_save_path + '.index'):\n",
    "    print('-------------load the model-----------------')\n",
    "    model.load_weights(checkpoint_save_path)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_save_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 save_best_only=True,\n",
    "                                                 monitor='val_loss')\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=64, epochs=50, validation_data=(x_test, y_test), validation_freq=1,\n",
    "                    callbacks=[cp_callback])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "file = open('C:/Users/Administrator/Desktop/GRU_RON_LOSS_weights.txt', 'w' )  # 参数提取\n",
    "for v in model.trainable_variables:\n",
    "    file.write(str(v.name) + '\\n')\n",
    "    file.write(str(v.shape) + '\\n')\n",
    "    file.write(str(v.numpy()) + '\\n')\n",
    "file.close()\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "################## predict ######################\n",
    "# 测试集输入模型进行预测\n",
    "predicted_RON_LOSS = model.predict(x_test)\n",
    "# 对预测数据还原---从（0，1）反归一化到原始范围\n",
    "predicted_RON_LOSS = sc_y.inverse_transform(predicted_RON_LOSS)\n",
    "# 对真实数据还原---从（0，1）反归一化到原始范围\n",
    "real_RON_LOSS = sc_y.inverse_transform(y_test)\n",
    "# 画出真实数据和预测数据的对比曲线\n",
    "plt.plot(real_RON_LOSS, color='red', label=' realed RON LOSS')\n",
    "plt.plot(predicted_RON_LOSS, color='blue', label='Predicted RON LOSS')\n",
    "plt.title('RON LOSS Prediction')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('RON LOSS')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "##########evaluate##############\n",
    "# calculate MSE 均方误差 ---> E[(预测值-真实值)^2] (预测值减真实值求平方后求均值)\n",
    "mse = mean_squared_error(predicted_RON_LOSS, real_RON_LOSS)\n",
    "# calculate RMSE 均方根误差--->sqrt[MSE]    (对均方误差开方)\n",
    "rmse = math.sqrt(mean_squared_error(predicted_RON_LOSS, real_RON_LOSS))\n",
    "# calculate MAE 平均绝对误差----->E[|预测值-真实值|](预测值减真实值求绝对值后求均值）\n",
    "mae = mean_absolute_error(predicted_RON_LOSS, real_RON_LOSS)\n",
    "print('均方误差: %.6f' % mse)\n",
    "print('均方根误差: %.6f' % rmse)\n",
    "print('平均绝对误差: %.6f' % mae)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data = pd.read_csv(r\"C:\\Users\\Administrator\\Desktop\\x_fe_poly.csv\")\n",
    "data = data.sort_values('id', ascending = False)\n",
    "all_samples = data.iloc[: , : ].values  # 所有样本的特征作为测试集\n",
    "\n",
    "# 归一化\n",
    "sc_x = MinMaxScaler(feature_range=(0, 1))  # 定义归一化：归一化到(0，1)之间\n",
    "sc_y = MinMaxScaler(feature_range=(0, 1))  # 定义归一化：归一化到(0，1)之间\n",
    "test_set_scaled_x = sc_x.fit_transform(all_samples[:,2:])  # 求得训练集的最大值，最小值这些训练集固有的属性，并在训练集上进行归一化\n",
    "test_set_scaled_y = sc_y.fit_transform(all_samples[:,1:2])  # 求得训练集的最大值，最小值这些训练集固有的属性，并在训练集上进行归一化\n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "for i in range(5, len(test_set_scaled_x)):\n",
    "    x_test.append(test_set_scaled_x[i - 5:i, :])\n",
    "    y_test.append(test_set_scaled_x[i, :])\n",
    "\n",
    "# 将训练集由list格式变为array格式\n",
    "x_test, y_test = np.array(x_test), np.array(y_test)\n",
    "\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], 5, 26))\n",
    "\n",
    "################## predict ######################\n",
    "# 测试集输入模型进行预测\n",
    "predicted_RON_LOSS = model.predict(x_test)\n",
    "# 对预测数据还原---从（0，1）反归一化到原始范围\n",
    "predicted_RON_LOSS = sc_y.inverse_transform(predicted_RON_LOSS)\n",
    "# 对真实数据还原---从（0，1）反归一化到原始范围\n",
    "real_RON_LOSS = all_samples[5:,1:2]\n",
    "\n",
    "# 画出真实数据和预测数据的对比曲线\n",
    "plt.plot(range(320,0,-1),real_RON_LOSS.ravel(), color='red', label=' realed RON LOSS')\n",
    "plt.plot(range(320,0,-1),predicted_RON_LOSS.ravel(), color='blue', label='Predicted RON LOSS')\n",
    "plt.title(' GRU of RON LOSS Prediction')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('RON LOSS')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "不在操作范围元素个数200,行数40,列数5 \n",
      " 列名['S-ZORB.SIS_LT_1001.PV', 'S-ZORB.FT_5201.PV', 'S-ZORB.FC_1101.TOTAL', 'S-ZORB.AI_2903.PV', 'S-ZORB.FT_1204.TOTAL']\n",
      "列名S-ZORB.SIS_LT_1001.PV,有超出范围的值有40个\n",
      "列名S-ZORB.FT_5201.PV,有超出范围的值有40个\n",
      "列名S-ZORB.FC_1101.TOTAL,有超出范围的值有40个\n",
      "列名S-ZORB.AI_2903.PV,有超出范围的值有40个\n",
      "列名S-ZORB.FT_1204.TOTAL,有超出范围的值有40个\n",
      "不在操作范围内大于5的列名['S-ZORB.SIS_LT_1001.PV', 'S-ZORB.FT_5201.PV', 'S-ZORB.FC_1101.TOTAL', 'S-ZORB.AI_2903.PV', 'S-ZORB.FT_1204.TOTAL'],有5个\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(40, 338)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### 导包和文件\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "w_285 = pd.read_csv(r\"C:\\Users\\Administrator\\Desktop\\285_02.csv\")\n",
    "y_285 = pd.read_csv(r\"C:\\Users\\Administrator\\Desktop\\285_range_01.csv\" )\n",
    "w_285.head()\n",
    "y_285.head()\n",
    "\n",
    "k = 0\n",
    "k_i = []\n",
    "k_j = []\n",
    "missing_285 = []\n",
    "for j in range(w_285.shape[1]):\n",
    "    for i in range(w_285.shape[0]):\n",
    "        if w_285.iloc[i,j] < y_285.iloc[j,3] or w_285.iloc[i,j] > y_285.iloc[j,4]:\n",
    "            w_285.iloc[i,j] = np.nan\n",
    "            k_j.append(w_285.columns[j])\n",
    "            k_i.append(w_285.index[i])\n",
    "            k+=1\n",
    "v_i = list(set(k_i))\n",
    "v_j = list(set(k_j))\n",
    "print(\"不在操作范围元素个数{},行数{},列数{} \\n 列名{}\".format(k,len(v_i),len(v_j),v_j))\n",
    "col = v_j\n",
    "for j in col:\n",
    "    w = w_285.loc[:,j]\n",
    "    h = np.isnan(w).sum()\n",
    "    print(\"列名{},有超出范围的值有{}个\".format(j,h))\n",
    "    if h > 5:\n",
    "        missing_285.append(j)\n",
    "print(\"不在操作范围内大于5的列名{},有{}个\".format(missing_285,len(missing_285)))\n",
    "\n",
    "w_285_copy = w_285\n",
    "w_285_copy.shape\n",
    "for i in missing_285:\n",
    "    w_285_copy.drop(columns = i , inplace = True)\n",
    "w_285_copy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "异常值元素个数0,行数0,列数0 \n",
      " 列名[]\n"
     ]
    }
   ],
   "source": [
    "#***********285标准化**********\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(w_285_copy)\n",
    "scaler.mean_\n",
    "scaler.var_\n",
    "w_285_copy_std = scaler.transform(w_285_copy)\n",
    "w_285_copy_std = pd.DataFrame(w_285_copy_std , columns = w_285_copy.columns)\n",
    "#*******判断285异常值并导出**********\n",
    "k = 0\n",
    "k_i = []\n",
    "k_j = []\n",
    "\n",
    "for j in range(w_285_copy_std.shape[1]):\n",
    "    for i in range(w_285_copy_std.shape[0]):\n",
    "        if w_285_copy_std.iloc[i,j] < -3 or w_285_copy_std.iloc[i,j] > 3:\n",
    "            w_285_copy.iloc[i,j] = np.nan\n",
    "            w_285_copy_std.iloc[i,j] = np.nan\n",
    "            k_j.append(w_285_copy_std.columns[j])\n",
    "            k_i.append(w_285_copy_std.index[i])\n",
    "            k+=1\n",
    "v_i = list(set(k_i))\n",
    "v_j = list(set(k_j))\n",
    "print(\"异常值元素个数{},行数{},列数{} \\n 列名{}\".format(k,len(v_i),len(v_j),v_j))\n",
    "col = v_j\n",
    "for j in col:\n",
    "    w = w_285_copy_std.loc[:,j]\n",
    "    h = np.isnan(w).sum()\n",
    "    print(\"列名{},有异常值{}个\".format(j,h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*********285取平均值********\n",
    "mean_285 = w_285_copy.mean(axis = 0)\n",
    "mean_285\n",
    "w_285_copy.to_csv(r\"C:\\Users\\Administrator\\Desktop\\285_isnan.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "不在操作范围元素个数283,行数40,列数27 \n",
      " 列名['S-ZORB.AT_5201.PV', 'S-ZORB.SIS_LT_1001.PV', 'S-ZORB.PC_2401.DACA', 'S-ZORB.FC_2501.PV', 'S-ZORB.PC_2401B.DACA', 'S-ZORB.PDT_2409.DACA', 'S-ZORB.AT-0006.DACA.PV', 'S-ZORB.AI_2903.PV', 'S-ZORB.PT_2502.DACA', 'S-ZORB.PC_2401B.PIDA.OP', 'S-ZORB.PT_2501.DACA', 'S-ZORB.PDC_2607.PV', 'S-ZORB.FT_9402.PV', 'S-ZORB.BS_LT_2401.PV', 'S-ZORB.PC_3101.DACA', 'S-ZORB.PC_2401.PIDA.SP', 'S-ZORB.AT-0012.DACA.PV', 'S-ZORB.TE_1603.DACA', 'S-ZORB.PDI_2301.DACA', 'S-ZORB.PDC_2502.PV', 'S-ZORB.FT_1204.TOTAL', 'S-ZORB.PDI_2801.DACA', 'S-ZORB.PT_9403.PV', 'S-ZORB.PT_6002.PV', 'S-ZORB.PC_6001.PV', 'S-ZORB.PC_2401B.PIDA.SP', 'S-ZORB.PC_2401.PIDA.OP']\n",
      "列名S-ZORB.AT_5201.PV,有超出范围的值有39个\n",
      "列名S-ZORB.SIS_LT_1001.PV,有超出范围的值有40个\n",
      "列名S-ZORB.PC_2401.DACA,有超出范围的值有4个\n",
      "列名S-ZORB.FC_2501.PV,有超出范围的值有4个\n",
      "列名S-ZORB.PC_2401B.DACA,有超出范围的值有3个\n",
      "列名S-ZORB.PDT_2409.DACA,有超出范围的值有2个\n",
      "列名S-ZORB.AT-0006.DACA.PV,有超出范围的值有7个\n",
      "列名S-ZORB.AI_2903.PV,有超出范围的值有40个\n",
      "列名S-ZORB.PT_2502.DACA,有超出范围的值有10个\n",
      "列名S-ZORB.PC_2401B.PIDA.OP,有超出范围的值有4个\n",
      "列名S-ZORB.PT_2501.DACA,有超出范围的值有8个\n",
      "列名S-ZORB.PDC_2607.PV,有超出范围的值有2个\n",
      "列名S-ZORB.FT_9402.PV,有超出范围的值有1个\n",
      "列名S-ZORB.BS_LT_2401.PV,有超出范围的值有14个\n",
      "列名S-ZORB.PC_3101.DACA,有超出范围的值有1个\n",
      "列名S-ZORB.PC_2401.PIDA.SP,有超出范围的值有6个\n",
      "列名S-ZORB.AT-0012.DACA.PV,有超出范围的值有3个\n",
      "列名S-ZORB.TE_1603.DACA,有超出范围的值有9个\n",
      "列名S-ZORB.PDI_2301.DACA,有超出范围的值有3个\n",
      "列名S-ZORB.PDC_2502.PV,有超出范围的值有20个\n",
      "列名S-ZORB.FT_1204.TOTAL,有超出范围的值有40个\n",
      "列名S-ZORB.PDI_2801.DACA,有超出范围的值有3个\n",
      "列名S-ZORB.PT_9403.PV,有超出范围的值有3个\n",
      "列名S-ZORB.PT_6002.PV,有超出范围的值有5个\n",
      "列名S-ZORB.PC_6001.PV,有超出范围的值有2个\n",
      "列名S-ZORB.PC_2401B.PIDA.SP,有超出范围的值有3个\n",
      "列名S-ZORB.PC_2401.PIDA.OP,有超出范围的值有7个\n",
      "不在操作范围内大于5的列名['S-ZORB.AT_5201.PV', 'S-ZORB.SIS_LT_1001.PV', 'S-ZORB.AT-0006.DACA.PV', 'S-ZORB.AI_2903.PV', 'S-ZORB.PT_2502.DACA', 'S-ZORB.PT_2501.DACA', 'S-ZORB.BS_LT_2401.PV', 'S-ZORB.PC_2401.PIDA.SP', 'S-ZORB.TE_1603.DACA', 'S-ZORB.PDC_2502.PV', 'S-ZORB.FT_1204.TOTAL', 'S-ZORB.PC_2401.PIDA.OP'],有12个\n"
     ]
    }
   ],
   "source": [
    "w_313 = pd.read_csv(r\"C:\\Users\\Administrator\\Desktop\\313_02.csv\")\n",
    "w_313 = w_313.drop(columns = \"id\")\n",
    "y_313 = pd.read_csv(r\"C:\\Users\\Administrator\\Desktop\\313_range_918_11.csv\" )\n",
    "k = 0\n",
    "k_i = []\n",
    "k_j = []\n",
    "missing_313 = []\n",
    "for j in range(w_313.shape[1]):\n",
    "    for i in range(w_313.shape[0]):\n",
    "        if w_313.iloc[i,j] < y_313.iloc[j,4] or w_313.iloc[i,j] > y_313.iloc[j,5]:\n",
    "            w_313.iloc[i,j] = np.nan\n",
    "            k_j.append(w_313.columns[j])\n",
    "            k_i.append(w_313.index[i])\n",
    "            k+=1\n",
    "v_i = list(set(k_i))\n",
    "v_j = list(set(k_j))\n",
    "print(\"不在操作范围元素个数{},行数{},列数{} \\n 列名{}\".format(k,len(v_i),len(v_j),v_j))\n",
    "col = v_j\n",
    "\n",
    "for j in col:\n",
    "    w = w_313.loc[:,j]\n",
    "    h = np.isnan(w).sum()\n",
    "    print(\"列名{},有超出范围的值有{}个\".format(j,h))\n",
    "    if h > 5:\n",
    "        missing_313.append(j)\n",
    "print(\"不在操作范围内大于5的列名{},有{}个\".format(missing_313,len(missing_313)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "异常值元素个数63,行数26,列数50 \n",
      " 列名['S-ZORB.FC_1203.PV', 'S-ZORB.PT_2101.PV', 'S-ZORB.FT_1204.DACA.PV', 'S-ZORB.CAL.LINE.PV', 'S-ZORB.PDT_2409.DACA', 'S-ZORB.FC_3103.PV', 'S-ZORB.PT_1201.PV', 'S-ZORB.TE_1107.DACA', 'S-ZORB.LT_1501.DACA', 'S-ZORB.FT_1204.PV', 'S-ZORB.PT_2106.DACA.PV', 'S-ZORB.PT_9402.PV', 'S-ZORB.PC_2105.PV', 'S-ZORB.AT-0010.DACA.PV', 'S-ZORB.PDT_1003.DACA', 'S-ZORB.PC_1202.PV', 'S-ZORB.PC_2401B.PIDA.OP', 'S-ZORB.PDT_2503.DACA', 'S-ZORB.FT_9401.PV', 'S-ZORB.AC_6001.PV', 'S-ZORB.PDC_2607.PV', 'S-ZORB.FC_2801.PV', 'S-ZORB.TE_1101.DACA', 'S-ZORB.PT_7107B.DACA', 'S-ZORB.ZT_2634.DACA', 'S-ZORB.PT_7103B.DACA', 'S-ZORB.PDT_2606.DACA', 'S-ZORB.TE_5008.DACA', 'S-ZORB.TE_1107.DACA.PV', 'S-ZORB.PT_7505B.DACA', 'S-ZORB.PT_1601.DACA', 'S-ZORB.FC_3101.PV', 'S-ZORB.PC_1301.PV', 'S-ZORB.LT_1002.DACA', 'S-ZORB.TE_1101.DACA.PV', 'S-ZORB.FT_9102.PV', 'S-ZORB.FT_3702.DACA', 'S-ZORB.PT_6002.PV', 'S-ZORB.PDT_3502.DACA', 'S-ZORB.PT_2607.DACA', 'S-ZORB.FC_1202.PV', 'S-ZORB.TE_1106.DACA', 'S-ZORB.FT_9302.PV', 'S-ZORB.AT-0011.DACA.PV', 'S-ZORB.AT-0013.DACA.PV', 'S-ZORB.PT_6003.DACA', 'S-ZORB.PT_1101.DACA', 'S-ZORB.PT_2106.DACA', 'S-ZORB.TE_1106.DACA.PV', 'S-ZORB.PT_7510B.DACA']\n"
     ]
    }
   ],
   "source": [
    "w_313_copy = w_313\n",
    "w_313_copy.shape\n",
    "for i in missing_313:\n",
    "    w_313_copy.drop(columns = i , inplace = True)\n",
    "w_313_copy.shape\n",
    "#***********313标准化并导出**********\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(w_313_copy)\n",
    "scaler.mean_\n",
    "scaler.var_\n",
    "w_313_copy_std = scaler.transform(w_313_copy)\n",
    "w_313_copy_std = pd.DataFrame(w_313_copy_std , columns = w_313_copy.columns)\n",
    "#*******判断313异常值********\n",
    "k = 0\n",
    "k_i = []\n",
    "k_j = []\n",
    "for j in range(w_313_copy_std.shape[1]):\n",
    "    for i in range(w_313_copy_std.shape[0]):\n",
    "        if w_313_copy_std.iloc[i,j] < -3 or w_313_copy_std.iloc[i,j] > 3:\n",
    "            w_313_copy.iloc[i,j] = np.nan\n",
    "            w_313_copy_std.iloc[i,j] = np.nan\n",
    "            k_j.append(w_313_copy_std.columns[j])\n",
    "            k_i.append(w_313_copy_std.index[i])\n",
    "            k+=1\n",
    "v_i = list(set(k_i))\n",
    "v_j = list(set(k_j))\n",
    "print(\"异常值元素个数{},行数{},列数{} \\n 列名{}\".format(k,len(v_i),len(v_j),v_j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
